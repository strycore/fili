#!/usr/bin/env python3
import os
import sys
import json
import hashlib
import binascii
import sqlite3

DBPATH = os.path.join(os.path.expanduser('~'), '.fili.db')
VIRTUAL_FS = [
    "/dev",
    "/sys",
    "/proc",
    "/run/user",
    "/run/udev",
    "/run/systemd",
    "/var/lib/flatpak",
]

CACHE_PATHS = {
    ".cache/mesa_shader_cache": "mesa",
    ".cache/thumbnails": "generic-image-thumbnails",
    ".cache/opera/Cache": "opera",
    ".cache/opera/Code Cache": "opera",
    ".config/opera/Service Worker/CacheStorage": "opera",
    ".cache/opera-developer/Cache": "opera-developer",
    ".cache/mozilla": "firefox",
    ".cache/pip": "pip",
    ".cache/pypoetry": "poetry",
    ".cache/shotwell/thumbs": "shotwell",
    ".cache/typescript": "typescript",
    ".config/Code/CachedData": "code",
    ".config/discord/Cache": "discord",
    ".config/discord/Code Cache": "discord",
    ".local/share/lutris/runners/wine": "lutris",
    ".node-gyp": "nodejs",
}

class db_cursor():
    def __enter__(self):
        self.db_conn = sqlite3.connect(DBPATH)
        cursor = self.db_conn.cursor()
        return cursor

    def __exit__(self, type, value, traceback):
        self.db_conn.commit()
        self.db_conn.close()


def create_db():
    db_conn = sqlite3.connect(DBPATH)
    cursor = db_conn.cursor()
    cursor.execute("""CREATE TABLE files
                   (path text,
                    size integer,
                    hash text,
                    hostname text,
                    accessed integer,
                    modified integer)""")
    db_conn.commit()
    db_conn.close()


def get_file_info(path):
    statinfo = os.stat(path)
    return {
        'path': path,
        'size': statinfo.st_size,
        'accessed': statinfo.st_atime,
        'modified': statinfo.st_mtime,
    }


def fastcheck(filename, length=8):
    """Generates a very basic file identifier in O(1) time."""
    size = os.path.getsize(filename)
    if size == 0:
        return None
    partsize = float(size) / float(length)
    fh = open(filename, 'r')
    output = ""
    for i in range(length):
        fh.seek(int(i * partsize))
        output += binascii.hexlify(fh.read(1))
    fh.close()
    return output


def index_path(path, with_hash=False):
    cache_paths = [os.path.join(os.path.expanduser("~"), cache_path) for cache_path in CACHE_PATHS]
    print(cache_paths)
    stats = {
        "indexed": 0,
        "skipped": 0,
        "failed": 0,
    }
    hostname = os.uname().nodename
    with db_cursor() as cursor:
        res = cursor.execute("DELETE FROM files WHERE path LIKE ?", (path + "%", ))
        print("Deleted %s entries" % res.rowcount)
        for filepath in iter_dir(path):
            skip_item = False
            for root_path in VIRTUAL_FS:
                if filepath.startswith(root_path):
                    skip_item = True
            for root_path in cache_paths:
                if filepath.startswith(root_path):
                    skip_item = True
            if skip_item:
                stats["skipped"] += 1
                continue
            if with_hash:
                filehash = calculate_md5(filepath)
            else:
                filehash = ""
            try:
                fileinfo = get_file_info(filepath)
            except (FileNotFoundError, PermissionError):
                print("Failed to read file info in %s" % filepath)
                stats["failed"] += 1
                continue
            try:
                cursor.execute(
                    "INSERT INTO files VALUES (?, ?, ?, ?, ?, ?)",
                    (fileinfo['path'], fileinfo['size'], filehash, hostname, fileinfo['accessed'], fileinfo['modified'])
                )
            except UnicodeEncodeError:
                print("Failed to save", fileinfo)
                stats["failed"] += 1
                continue
            stats["indexed"] += 1
            if stats["indexed"] % 1000 == 0:
                print("Indexed %s files. (Current file: %s)" % (stats["indexed"], filepath))
        print("Indexed %s files" % stats["indexed"])
        print("Skipped %s files" % stats["skipped"])
        print("Failed %s files" % stats["failed"])


def iter_dupes():
    with db_cursor() as cursor:
        dupes = cursor.execute("""SELECT count(path), hash
                                  FROM files GROUP BY hash
                                  HAVING count(path) > 1 AND hash != '0'
                                  ORDER BY path""")
        for dupehash in dupes.fetchall():
            filehash = dupehash[1]
            dupe_files = cursor.execute("""SELECT * FROM files WHERE hash=?""",
                                        (filehash, ))
            yield dupe_files.fetchall()


def get_recent():
    with db_cursor() as cursor:
        recent_files = cursor.execute(
            """SELECT * FROM files
            WHERE path LIKE '/home%' AND path NOT LIKE '%/.%'
            ORDER BY accessed DESC
            LIMIT 100"""
        )
        for recent in recent_files.fetchall():
            print(recent)



def delete_dupes():
    for dupe_group in iter_dupes():
        for index, dupefile in enumerate(dupe_group):
            filepath = dupefile[0].encode('utf-8')
            if index == 0:
                print("keeping " + filepath)
            else:
                print("deleting " + filepath)
                try:
                    os.remove(filepath)
                except OSError:
                    print("Can't remove file %s" % filepath)
                unindex(filepath, strict=True)


def print_dupes():
    for dupe_group in iter_dupes():
        for dupefile in dupe_group:
            print(dupefile[0].encode('utf-8'))


def search_file(query):
    with db_cursor() as cursor:
        result = cursor.execute("SELECT * FROM files WHERE path LIKE '%s'" %
                                ('%' + query + '%'))
        for fileentry in result.fetchall():
            print(fileentry[0].encode('utf-8'))


def unindex(path_query, strict=False):
    glob = '' if strict else '%'
    with db_cursor() as cursor:
        res = cursor.execute("DELETE FROM files WHERE path LIKE ?", (path_query + glob, ))
        print("%s files unindexed" % res.rowcount)


def calculate_md5(filename):
    md5 = hashlib.md5()
    try:
        with open(filename, 'rb') as f:
            for chunk in iter(lambda: f.read(8192), b''):
                md5.update(chunk)
    except IOError:
        print("Error reading %s" % filename)
        return False
    return md5.hexdigest()


def iter_dir(path):
    for root, dirs, files in os.walk(path):
        for filename in files:
            yield os.path.join(root, filename)


def export_files(export_path="results.json"):
    all_data = []
    with db_cursor() as cursor:
        recent_files = cursor.execute(
            """SELECT * FROM files"""
        )
        for recent in recent_files.fetchall():
            all_data.append({
                "path": str(recent[0]),
                "size": recent[1],
                "hash": recent[2],
                "host": recent[3],
                "atime": recent[4],
                "mtime": recent[5]
            })
    with open(export_path, "w", encoding="utf-8") as result_file:
        json.dump(all_data, result_file, indent=2)


if __name__== "__main__":
    if len(sys.argv) < 2:
        print("No command given")
        exit(2)
    if not os.path.exists(DBPATH):
        create_db()
    if sys.argv[1] == "index":
        index_path(sys.argv[2])
    if sys.argv[1] == "dupes":
        print_dupes()
    if sys.argv[1] == "delete-dupes":
        delete_dupes()
    if sys.argv[1] == "unindex":
        unindex(sys.argv[2])
    if sys.argv[1] == "search":
        search_file(sys.argv[2])
    if sys.argv[1] == "recent":
        get_recent()
    if sys.argv[1] == "export":
        export_files()